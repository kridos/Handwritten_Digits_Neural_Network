# MNIST Neural Network

## Project Overview

This project involves the development of a foundational multilayer perceptron (MLP) neural network built from scratch in Python. The network is designed to accurately classify handwritten digits from the widely-used MNIST dataset, demonstrating a deep understanding of core machine learning principles.

## Features

* **Multilayer Perceptron (MLP) Architecture:** Implemented a feedforward neural network capable of learning complex patterns.

* **Backpropagation Algorithm:** Utilized backpropagation for efficient calculation of gradients and error propagation through the network.

* **Gradient Descent Optimization:** Employed gradient descent to iteratively update network weights and biases, minimizing the loss function.

* **MNIST Dataset Integration:** Processed and trained the network on the standard MNIST dataset for handwritten digit recognition.

* **From Scratch Implementation:** Built all core components of the neural network without relying on high-level deep learning frameworks, showcasing fundamental understanding.

## Technologies Used

* **Python:** The primary programming language for the entire implementation.

* **NumPy:** Used extensively for numerical operations, array manipulation, and efficient mathematical computations essential for neural network calculations.

## My Contributions (In Progress)

* Designed and implemented the full neural network architecture, including input, hidden, and output layers.

* Developed the forward and backward propagation algorithms.

* Wrote the training loop, including loss calculation and weight updates using gradient descent.

* Handled data loading and preprocessing for the MNIST dataset.

## Potential Future Enhancements

* Implement additional activation functions (e.g., ReLU, Leaky ReLU).

* Add support for different optimizers (e.g., Adam, SGD with momentum).

* Integrate a convolutional layer for improved performance on image data.

* Containerize the application using Docker.
